# Principles of Machine Learning in Python

This course is offered by Microsoft and officially available on [EdX](https://www.edx.org/course/principles-of-machine-learning-python-edition-4)[.](https://github.com/MicrosoftLearning/Principles-of-Machine-Learning-Python)

However, I have added multiple new features in some of the notebooks to make it easier to understand as well as visualize.

# Course Syllabus
- Introduction to Machine Learning
- Data Exploration, Preparation and Cleaning
- Introduction to Supervised Learning
- Improving Model Performance
- Machine Learning Algorithms
- Unsupervised Learning

# Notebooks   

**1. IntroductionToMachineLearning**
  - KNN Classification

**2. VisualizingDataForClassification**
  - Box plot, Violin plot

**3. VisualizingDataForRegression**
  - Box plot, Violin plot
  - Combined Histogram, KDEs, 2D density plot, Pair plot

**4. DataPreparation**
  - Feature Engineering

**5. ApplyingLinearRegression**
  - Construction and evaluation of LR model

**6. Classification**
  - Confusion matrix, Accuracy, Precision, Recall, F1, ROC, AUC
  - Weighted Model

**7. IntroductionToRegression**
  - Errors: MSE, RMSE, Mean(Median) Absolute Error
  - R Squared, adjusted R Squared

**8. Bias-Variance-Trade-Off**
  - L1, L2 Regularization

**9. CrossValidation**
  - K-Fold Cross Validation
  - Nested Cross Validation

**10. DimensionalityReduction**
  - Principle component analysis

**11. FeatureSelection**
  - Eliminate Low Variance Features
  - Select k best features

**12. Bagging**
  - Random Forest Model

**13. Boosting**
  - Boosting methods: AdaBoost

**14. NaiveBayes**
  - Gaussian, Bernoulli, Multinomial
     
**15. NeuralNetworks**
  - Optimal Hyperparameters

**16. SupportVectorMachines**
  - Linear Kernel, Radial Basis Function (RBF) Kernel

**17. ApplicationOfClustering**
  - Kmeans Clustering
  - Agglomerative Clustering

**18. IntroToUnsupervisedLearning**
  - Kmeans Clustering
  - Hierarchical Clustering: Linkage Distance (ward, average, complete)

